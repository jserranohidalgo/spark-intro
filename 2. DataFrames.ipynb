{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/03/04 15:09:57 INFO SparkContext: Running Spark version 2.4.3\n",
      "20/03/04 15:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/03/04 15:09:58 INFO SparkContext: Submitted application: c759fc7c-e743-423a-bfd0-79c8689b52ee\n",
      "20/03/04 15:09:58 INFO SecurityManager: Changing view acls to: jserrano\n",
      "20/03/04 15:09:58 INFO SecurityManager: Changing modify acls to: jserrano\n",
      "20/03/04 15:09:58 INFO SecurityManager: Changing view acls groups to: \n",
      "20/03/04 15:09:58 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/03/04 15:09:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jserrano); groups with view permissions: Set(); users  with modify permissions: Set(jserrano); groups with modify permissions: Set()\n",
      "20/03/04 15:09:59 INFO Utils: Successfully started service 'sparkDriver' on port 51020.\n",
      "20/03/04 15:09:59 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/03/04 15:09:59 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/03/04 15:09:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/03/04 15:09:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/03/04 15:09:59 INFO DiskBlockManager: Created local directory at /private/var/folders/0n/mm44trks5_96srf708_htq140000gn/T/blockmgr-ef5c67ff-059f-4575-94fc-13f2f4f110ec\n",
      "20/03/04 15:09:59 INFO MemoryStore: MemoryStore started with capacity 912.3 MB\n",
      "20/03/04 15:09:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/03/04 15:10:00 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "20/03/04 15:10:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.84.117:4042\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0-sources.jar at spark://10.0.84.117:51020/jars/jvm-repr-0.4.0-sources.jar with timestamp 1583331000378\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://10.0.84.117:51020/jars/jvm-repr-0.4.0.jar with timestamp 1583331000381\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1-sources.jar at spark://10.0.84.117:51020/jars/scopt_2.12-3.7.1-sources.jar with timestamp 1583331000382\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar at spark://10.0.84.117:51020/jars/scopt_2.12-3.7.1.jar with timestamp 1583331000383\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.9/1.7.4/ammonite-interp-api_2.12.9-1.7.4-sources.jar at spark://10.0.84.117:51020/jars/ammonite-interp-api_2.12.9-1.7.4-sources.jar with timestamp 1583331000384\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.9/1.7.4/ammonite-interp-api_2.12.9-1.7.4.jar at spark://10.0.84.117:51020/jars/ammonite-interp-api_2.12.9-1.7.4.jar with timestamp 1583331000386\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4-sources.jar at spark://10.0.84.117:51020/jars/ammonite-ops_2.12-1.7.4-sources.jar with timestamp 1583331000388\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4.jar at spark://10.0.84.117:51020/jars/ammonite-ops_2.12-1.7.4.jar with timestamp 1583331000401\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.9/1.7.4/ammonite-repl-api_2.12.9-1.7.4-sources.jar at spark://10.0.84.117:51020/jars/ammonite-repl-api_2.12.9-1.7.4-sources.jar with timestamp 1583331000403\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.9/1.7.4/ammonite-repl-api_2.12.9-1.7.4.jar at spark://10.0.84.117:51020/jars/ammonite-repl-api_2.12.9-1.7.4.jar with timestamp 1583331000410\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4-sources.jar at spark://10.0.84.117:51020/jars/ammonite-util_2.12-1.7.4-sources.jar with timestamp 1583331000411\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4.jar at spark://10.0.84.117:51020/jars/ammonite-util_2.12-1.7.4.jar with timestamp 1583331000412\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7-sources.jar at spark://10.0.84.117:51020/jars/fansi_2.12-0.2.7-sources.jar with timestamp 1583331000413\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7.jar at spark://10.0.84.117:51020/jars/fansi_2.12-0.2.7.jar with timestamp 1583331000413\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8-sources.jar at spark://10.0.84.117:51020/jars/geny_2.12-0.1.8-sources.jar with timestamp 1583331000414\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8.jar at spark://10.0.84.117:51020/jars/geny_2.12-0.1.8.jar with timestamp 1583331000424\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0-sources.jar at spark://10.0.84.117:51020/jars/os-lib_2.12-0.3.0-sources.jar with timestamp 1583331000426\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0.jar at spark://10.0.84.117:51020/jars/os-lib_2.12-0.3.0.jar with timestamp 1583331000430\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5-sources.jar at spark://10.0.84.117:51020/jars/pprint_2.12-0.5.5-sources.jar with timestamp 1583331000431\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5.jar at spark://10.0.84.117:51020/jars/pprint_2.12-0.5.5.jar with timestamp 1583331000432\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7-sources.jar at spark://10.0.84.117:51020/jars/sourcecode_2.12-0.1.7-sources.jar with timestamp 1583331000433\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7.jar at spark://10.0.84.117:51020/jars/sourcecode_2.12-0.1.7.jar with timestamp 1583331000435\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12-sources.jar at spark://10.0.84.117:51020/jars/interface-0.0.12-sources.jar with timestamp 1583331000436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12.jar at spark://10.0.84.117:51020/jars/interface-0.0.12.jar with timestamp 1583331000438\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0-sources.jar at spark://10.0.84.117:51020/jars/scala-collection-compat_2.12-2.0.0-sources.jar with timestamp 1583331000440\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0.jar at spark://10.0.84.117:51020/jars/scala-collection-compat_2.12-2.0.0.jar with timestamp 1583331000442\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0-sources.jar at spark://10.0.84.117:51020/jars/scala-xml_2.12-1.2.0-sources.jar with timestamp 1583331000449\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar at spark://10.0.84.117:51020/jars/scala-xml_2.12-1.2.0.jar with timestamp 1583331000449\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.9/scala-compiler-2.12.9-sources.jar at spark://10.0.84.117:51020/jars/scala-compiler-2.12.9-sources.jar with timestamp 1583331000450\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.9/scala-library-2.12.9-sources.jar at spark://10.0.84.117:51020/jars/scala-library-2.12.9-sources.jar with timestamp 1583331000467\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.9/scala-reflect-2.12.9-sources.jar at spark://10.0.84.117:51020/jars/scala-reflect-2.12.9-sources.jar with timestamp 1583331000468\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.8.2/interpreter-api_2.12-0.8.2-sources.jar at spark://10.0.84.117:51020/jars/interpreter-api_2.12-0.8.2-sources.jar with timestamp 1583331000469\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.8.2/interpreter-api_2.12-0.8.2.jar at spark://10.0.84.117:51020/jars/interpreter-api_2.12-0.8.2.jar with timestamp 1583331000473\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.8.2/jupyter-api_2.12-0.8.2-sources.jar at spark://10.0.84.117:51020/jars/jupyter-api_2.12-0.8.2-sources.jar with timestamp 1583331000474\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.8.2/jupyter-api_2.12-0.8.2.jar at spark://10.0.84.117:51020/jars/jupyter-api_2.12-0.8.2.jar with timestamp 1583331000475\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.9/0.8.2/scala-kernel-api_2.12.9-0.8.2-sources.jar at spark://10.0.84.117:51020/jars/scala-kernel-api_2.12.9-0.8.2-sources.jar with timestamp 1583331000475\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.9/0.8.2/scala-kernel-api_2.12.9-0.8.2.jar at spark://10.0.84.117:51020/jars/scala-kernel-api_2.12.9-0.8.2.jar with timestamp 1583331000476\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Jupyter/kernels/scala212/launcher.jar at spark://10.0.84.117:51020/jars/launcher.jar with timestamp 1583331000477\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.8/scala-reflect-2.12.8.jar at spark://10.0.84.117:51020/jars/scala-reflect-2.12.8.jar with timestamp 1583331000487\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar at spark://10.0.84.117:51020/jars/scala-library-2.12.10.jar with timestamp 1583331000488\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.19.v20190610/jetty-http-9.4.19.v20190610.jar at spark://10.0.84.117:51020/jars/jetty-http-9.4.19.v20190610.jar with timestamp 1583331000491\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.19.v20190610/jetty-server-9.4.19.v20190610.jar at spark://10.0.84.117:51020/jars/jetty-server-9.4.19.v20190610.jar with timestamp 1583331000492\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.19.v20190610/jetty-util-9.4.19.v20190610.jar at spark://10.0.84.117:51020/jars/jetty-util-9.4.19.v20190610.jar with timestamp 1583331000493\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.6.0/almond-spark_2.12-0.6.0.jar at spark://10.0.84.117:51020/jars/almond-spark_2.12-0.6.0.jar with timestamp 1583331000494\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.5.0/ammonite-spark_2.12-0.5.0.jar at spark://10.0.84.117:51020/jars/ammonite-spark_2.12-0.5.0.jar with timestamp 1583331000495\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.19.v20190610/jetty-io-9.4.19.v20190610.jar at spark://10.0.84.117:51020/jars/jetty-io-9.4.19.v20190610.jar with timestamp 1583331000496\n",
      "20/03/04 15:10:00 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.8/scala-library-2.12.8.jar at spark://10.0.84.117:51020/jars/scala-library-2.12.8.jar with timestamp 1583331000497\n",
      "20/03/04 15:10:00 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/03/04 15:10:01 INFO Executor: Using REPL class URI: http://10.0.84.117:51019\n",
      "20/03/04 15:10:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51021.\n",
      "20/03/04 15:10:01 INFO NettyBlockTransferService: Server created on 10.0.84.117:51021\n",
      "20/03/04 15:10:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/03/04 15:10:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.84.117, 51021, None)\n",
      "20/03/04 15:10:01 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.84.117:51021 with 912.3 MB RAM, BlockManagerId(driver, 10.0.84.117, 51021, None)\n",
      "20/03/04 15:10:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.84.117, 51021, None)\n",
      "20/03/04 15:10:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.84.117, 51021, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.0.84.117:4042\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msparksession._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._, func._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create datasets from external data sources using different formats, e.g. Json, parquet, CSV, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd6.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeople\u001b[39m: \u001b[32mDataFrame\u001b[39m = [age: bigint, name: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people: DataFrame = spark.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we created a `DataFrame`, not a `Dataset`. Dataframes are like datasets, i.e. programs to generate distributed data sets, but *dynamically typed*. This means that, at compile time, Scala only knows that a dataframe consists of `Row`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]\n",
       "\u001b[36mres9_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds: Dataset[Int] = List(1,2,3).toDS\n",
    "ds.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd10.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [2000,YiHui],\n",
       "  [2000,Javier],\n",
       "  [2003,Gabriel],\n",
       "  [2004,Noelia]\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, a `DataFrame` is defined as an alias of `Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeopleDs\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [age: bigint, name: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDs: Dataset[Row] = people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the type of the information to be processed is there! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13_0\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"age\"\u001b[39m, LongType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"name\"\u001b[39m, StringType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.schema\n",
    "people.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can convert a dataframe into a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mPerson\u001b[39m\n",
       "\u001b[36mpeopleDs\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPerson\u001b[39m] = [age: bigint, name: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val peopleDs: Dataset[Person] = people.as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">i</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Int</span></span> = <span style=\"color: white\"><span class=\"ansi-white-fg\">[lazy]</span></span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mi\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[37m[lazy]\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val i: Int = {println(\"hola\"); 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres22\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m2\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`name1`' given input columns: [age, name];\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m110\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m295\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m354\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m392\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m231\u001b[39m)\n  scala.collection.immutable.List.map(\u001b[32mList.scala\u001b[39m:\u001b[32m298\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m354\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m82\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n  org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(\u001b[32mExpressionEncoder.scala\u001b[39m:\u001b[32m258\u001b[39m)\n  org.apache.spark.sql.Dataset.deserializer$lzycompute(\u001b[32mDataset.scala\u001b[39m:\u001b[32m213\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer(\u001b[32mDataset.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  org.apache.spark.sql.Dataset$.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  org.apache.spark.sql.Dataset.as(\u001b[32mDataset.scala\u001b[39m:\u001b[32m430\u001b[39m)\n  ammonite.$sess.cmd25$Helper.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m6\u001b[39m)\n  ammonite.$sess.cmd25$.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd25$.<clinit>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Person(name1: String, age: Long)\n",
    "\n",
    "// def peopleDs: Dataset[Person1] = people.as[Person1]\n",
    "people.as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`name1`' given input columns: [age, name];\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m110\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m295\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m354\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m392\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m231\u001b[39m)\n  scala.collection.immutable.List.map(\u001b[32mList.scala\u001b[39m:\u001b[32m298\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m354\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m275\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m82\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n  org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(\u001b[32mExpressionEncoder.scala\u001b[39m:\u001b[32m258\u001b[39m)\n  org.apache.spark.sql.Dataset.deserializer$lzycompute(\u001b[32mDataset.scala\u001b[39m:\u001b[32m213\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$deserializer(\u001b[32mDataset.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  org.apache.spark.sql.Dataset$.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  org.apache.spark.sql.Dataset.as(\u001b[32mDataset.scala\u001b[39m:\u001b[32m430\u001b[39m)\n  ammonite.$sess.cmd23$Helper.peopleDs(\u001b[32mcmd23.sc\u001b[39m:\u001b[32m5\u001b[39m)\n  ammonite.$sess.cmd26$Helper.<init>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd26$.<init>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd26$.<clinit>(\u001b[32mcmd26.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "peopleDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mi\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m0\u001b[39m\n",
       "\u001b[36mj\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0\"\u001b[39m\n",
       "\u001b[36mres17_2\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m0\u001b[39m\n",
       "\u001b[36mres17_3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"0\"\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val i: Int = 0\n",
    "val j: String = i.toString\n",
    "\n",
    "i\n",
    "j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd14.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2000|  YiHui|\n",
      "|2000| Javier|\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd14.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2000|  YiHui|\n",
      "|2000| Javier|\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDs.show\n",
    "people.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untyped transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API includes a section on _untyped transformations_. These are transformations that are not defined over the Scala types but over the inner Spark SQL types (i.e. `StructType`s). More exactly, these could be named *dynamically typed transformations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations are in close corresponde with their SQL counterparts: `SELECT`, `WHERE`, `GROUP BY`, `FROM`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the equivalent to the `map` typed transformation is `select`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd29.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd29.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|  YiHui|\n",
      "| Javier|\n",
      "|Gabriel|\n",
      "| Noelia|\n",
      "+-------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#103]\n",
      "+- *(1) MapElements ammonite.$sess.cmd29$Helper$$Lambda$5286/96895532@56768c7b, obj#102: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd27$Helper$Person), obj#101: ammonite.$sess.cmd27$Helper$Person\n",
      "      +- *(1) FileScan json [age#48L,name#49] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/Users/jserrano/Documents/spark-intro/data/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]\n",
       "\u001b[36mres29_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"YiHui\"\u001b[39m, \u001b[32m\"Javier\"\u001b[39m, \u001b[32m\"Gabriel\"\u001b[39m, \u001b[32m\"Noelia\"\u001b[39m)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds: Dataset[String] = peopleDs.map(_.name)\n",
    "ds.collect\n",
    "ds.show\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd30.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd30.sc:4</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  YiHui|\n",
      "| Javier|\n",
      "|Gabriel|\n",
      "| Noelia|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string]\n",
       "\u001b[36mres30_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([YiHui], [Javier], [Gabriel], [Noelia])\n",
       "\u001b[36mres30_3\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\u001b[33mStructField\u001b[39m(\u001b[32m\"name\"\u001b[39m, StringType, true, {}))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df: DataFrame = \n",
    "    people.select($\"name\")\n",
    "df.collect\n",
    "df.show\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we lost the column label (`name`) in the case of the dataset transformation. This is not happening with `select`. Moreover, we have more control over the resulting schema: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mt3\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mBoolean\u001b[39m) = (\u001b[32m\"\"\u001b[39m, \u001b[32m1\u001b[39m, false)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t3: Tuple3[String, Int, Boolean] = (\"\",1,false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres39_0\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1\u001b[39m\n",
       "\u001b[36mres39_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\u001b[39m\n",
       "\u001b[36mres39_2\u001b[39m: \u001b[32mBoolean\u001b[39m = false"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3._2\n",
    "t3._1\n",
    "t3._3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd34.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+\n",
      "|     _1|  _2| _3|\n",
      "+-------+----+---+\n",
      "|  YiHui|2001|YiH|\n",
      "| Javier|2001|Jav|\n",
      "|Gabriel|2004|Gab|\n",
      "| Noelia|2005|Noe|\n",
      "+-------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDs.map((p: Person) => (p.name, ((p.age: Int) + 1): Int, p.name.substring(0,3)))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd44.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|prefix|\n",
      "+-------+----+------+\n",
      "|  YiHui|2001|   YiH|\n",
      "| Javier|2001|   Jav|\n",
      "|Gabriel|2004|   Gab|\n",
      "| Noelia|2005|   Noe|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(col(\"name\"), \n",
    "              (col(\"age\").+(lit(1))) as \"age\", \n",
    "              col(\"name\").substr(0,3) as \"prefix\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd47.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|prefix|\n",
      "+-------+----+------+\n",
      "|  YiHui|2001|   YiH|\n",
      "| Javier|2001|   Jav|\n",
      "|Gabriel|2004|   Gab|\n",
      "| Noelia|2005|   Noe|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select($\"name\", \n",
    "              ($\"age\" + 1) as \"age\", \n",
    "              $\"name\".substr(0,3) as \"prefix\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = <style>@keyframes fadein { from { opacity: 0; } to { opacity: 1; } }</style><span style=\"animation: fadein 2s;\">[name: string, age: string ... 2 more fields]</span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, age: string ... 2 more fields]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val df = people.select($\"name\", \n",
    "              (concat($\"name\",lit(\"_a\")): Column) as \"age\", \n",
    "              ($\"name\".substr(0,3): Column) as \"prefix\",\n",
    "              $\"age\" > 2001 as \"c2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd83.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres83\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [YiHui,YiHui_a,YiH,false],\n",
       "  [Javier,Javier_a,Jav,false],\n",
       "  [Gabriel,Gabriel_a,Gab,true],\n",
       "  [Noelia,Noelia_a,Noe,true]\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) contains dozens of column operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _untyped_, or more properly, _dynamically typed_, character means that the Scala compiler won't complain if we choose a non-existent column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd77.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [age: bigint, name: string]\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [agee: bigint, nam: string]\n",
       "\u001b[36mres77_2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [nam: string]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df: DataFrame = spark.read.json(\"data/people.json\")\n",
    "val df2: DataFrame = df.toDF(\"agee\",\"nam\")\n",
    "df2.select($\"nam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`nam`' given input columns: [age, name];;\n'Project ['nam]\n+- Relation[age#462L,name#463] json\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m110\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m231\u001b[39m)\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m82\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m47\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3406\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1335\u001b[39m)\n  ammonite.$sess.cmd79$Helper.<init>(\u001b[32mcmd79.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd79$.<init>(\u001b[32mcmd79.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd79$.<clinit>(\u001b[32mcmd79.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.select($\"nam\").explain\n",
    "df2.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error will be shown at runtime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd69.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres70\u001b[39m: \u001b[32mDataFrame\u001b[39m = [nam: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary, the error in the dataset transformation manifests at compile-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd76.sc:1: value nam is not a member of cmd76.this.cmd27.Person\n",
      "val res76 = (peopleDs: Dataset[Person]).map(_.nam)\n",
      "                                              ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "(peopleDs: Dataset[Person]).map(_.nam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent to the typed `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres81\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPerson\u001b[39m] = [age: bigint, name: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDs.filter(_.age > 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd80.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.filter($\"age\" > 2001)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a column function not denoting a boolean value, we won't even get a run-time exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf\u001b[39m"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df: DataFrame = \n",
    "    people.filter($\"name\" > \"Javier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd91.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| age|  name|\n",
      "+----+------+\n",
      "|2000| YiHui|\n",
      "|2004|Noelia|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `groupBy` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd92.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstudents\u001b[39m: \u001b[32mDataFrame\u001b[39m = [degree: string, name: string]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val students: DataFrame = spark.read.json(\"data/students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    4 / 4\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    20 / 20\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    39 / 39\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|degree|count|\n",
      "+------+-----+\n",
      "|   MAT|    2|\n",
      "|    II|    4|\n",
      "|    IS|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.groupBy($\"degree\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">pivot at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">pivot at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    64 / 64\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">pivot at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    64 / 64\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    4 / 4\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    20 / 20\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd136.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    39 / 39\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+-----+\n",
      "|degree|Gabriel|Javier|Noelia|YiHui|\n",
      "+------+-------+------+------+-----+\n",
      "|   MAT|   null|     1|  null|    1|\n",
      "|    II|      1|     1|     1|    1|\n",
      "|    IS|   null|  null|     1| null|\n",
      "+------+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.groupBy($\"degree\").pivot($\"name\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    4 / 4\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    20 / 20\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd93.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    39 / 39\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|degree|count|\n",
      "+------+-----+\n",
      "|   MAT|    2|\n",
      "|    II|    4|\n",
      "|    IS|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students.groupBy($\"degree\").count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join` transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed joins, but we didn't mention that the resulting type of a join is a dataframe, not a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mStudent\u001b[39m"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Student(name: String, degree: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres97\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, age: bigint ... 1 more field]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDs.join(students.as[Student], \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">run at ThreadPoolExecutor.java:1142</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd99.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|degree|\n",
      "+-------+----+------+\n",
      "|  YiHui|2000|    II|\n",
      "|  YiHui|2000|   MAT|\n",
      "| Javier|2000|    II|\n",
      "| Javier|2000|   MAT|\n",
      "|Gabriel|2003|    II|\n",
      "| Noelia|2004|    II|\n",
      "| Noelia|2004|    IS|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res97.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">run at ThreadPoolExecutor.java:1142</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd103.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "| age|degree|   name|\n",
      "+----+------+-------+\n",
      "|2000|    II|  YiHui|\n",
      "|2000|   MAT|  YiHui|\n",
      "|2000|    II| Javier|\n",
      "|2000|   MAT| Javier|\n",
      "|2003|    II|Gabriel|\n",
      "|2004|    II| Noelia|\n",
      "|2004|    IS| Noelia|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDs.join(students.as[Student], \n",
    "              people.col(\"name\").equalTo(students.col(\"name\")))\n",
    "    .drop(people.col(\"name\"))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "|age|degree|name|\n",
      "+---+------+----+\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(students, \n",
    "              lit(false)) // people.col(\"name\").equalTo(lit(\"\")))\n",
    "    .drop(people.col(\"name\"))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">run at ThreadPoolExecutor.java:1142</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd107.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|degree|\n",
      "+-------+----+------+\n",
      "|  YiHui|2000|    II|\n",
      "|  YiHui|2000|   MAT|\n",
      "| Javier|2000|    II|\n",
      "| Javier|2000|   MAT|\n",
      "|Gabriel|2003|    II|\n",
      "| Noelia|2004|    II|\n",
      "| Noelia|2004|    IS|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(students,\"name\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problems of `Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are nice because they are type safe, but, unfortunately, they are less efficient than data frames in several respects. This can be best shown by reading from parquet source files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a _columnar_ format, which means that it stores physically data around columns, allowing us to read only data from a particular column without reading the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">parquet at cmd115.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "people.write.mode(\"overwrite\").parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">parquet at cmd117.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd117.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2000|  YiHui|\n",
      "|2000| Javier|\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"data/people.parquet\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ReadSchema` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a program that simply read the _name_ column of the people dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#945]\n",
      "+- *(1) MapElements ammonite.$sess.cmd122$Helper$$Lambda$6471/1725825756@5a64274c, obj#944: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd27$Helper$Person), obj#943: ammonite.$sess.cmd27$Helper$Person\n",
      "      +- LocalTableScan [name#939, age#940L]\n"
     ]
    }
   ],
   "source": [
    "List(Person(\"a\",1), Person(\"b\", 2)).toDS\n",
    "    .map((t: Person) => t.name).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">parquet at cmd123.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds: Dataset[String] = \n",
    "    spark.read.parquet(\"data/people.parquet\").as[Person]\n",
    "        .map(_.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#923]\n",
      "+- *(1) MapElements ammonite.$sess.cmd118$Helper$$Lambda$6460/1380480315@26e34f85, obj#922: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd27$Helper$Person), obj#921: ammonite.$sess.cmd27$Helper$Person\n",
      "      +- *(1) FileScan parquet [age#913L,name#914] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jserrano/Documents/spark-intro/data/people.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    }
   ],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works as intended: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd124.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|  YiHui|\n",
      "| Javier|\n",
      "|Gabriel|\n",
      "| Noelia|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, however: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#958]\n",
      "+- *(1) MapElements ammonite.$sess.cmd123$Helper$$Lambda$6474/322703264@37c1d315, obj#957: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd27$Helper$Person), obj#956: ammonite.$sess.cmd27$Helper$Person\n",
      "      +- *(1) FileScan parquet [age#948L,name#949] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jserrano/Documents/spark-intro/data/people.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    }
   ],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plan includes the directive `ReadSchema: struct<age:bigint,name:string>`, which generates a query to scan the full schema of the parquet file. But we just want to read the names! We can create an optimun program using dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">parquet at cmd133.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, concat(name, aaa): string ... 1 more field]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.parquet(\"data/people.parquet\")\n",
    "        .select($\"name\", concat($\"name\", lit(\"aaa\")), $\"age\" + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd134.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+---------+\n",
      "|   name|concat(name, aaa)|(age + 1)|\n",
      "+-------+-----------------+---------+\n",
      "|  YiHui|         YiHuiaaa|     2001|\n",
      "| Javier|        Javieraaa|     2001|\n",
      "|Gabriel|       Gabrielaaa|     2004|\n",
      "| Noelia|        Noeliaaaa|     2005|\n",
      "+-------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but more efficiently (note the the value of the `ReadSchema` directive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [name#995, concat(name#995, aaa) AS concat(name, aaa)#998, (age#994L + 1) AS (age + 1)#999L]\n",
      "+- *(1) FileScan parquet [age#994L,name#995] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/jserrano/Documents/spark-intro/data/people.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    }
   ],
   "source": [
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can empirically check that it actually works using the Spark UI. First, we create a parquet file with enough rows and several columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read the second column using both datasets and dataframes, and check the Spark UI for the _Input Size_ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test = spark.read.parquet(\"data/test\")\n",
    "test.as[Tuple2[Long, Int]].map(_._2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataframes the input size is much lower since we only read the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select($\"_2\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PushedFilter` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following equivalent dataset and dataframe programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[(Long, Int)] = \n",
    "    test.as[(Long, Int)]\n",
    "        .filter(_._1 >= 999995)\n",
    "\n",
    "val df: DataFrame = \n",
    "    test\n",
    "        .filter($\"_1\" >= 999995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, they are equivalent, but their performance differ significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect\n",
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this difference lies in another optimization applied by the Spark SQL compiler: the so-called push-down filter optimization. In the previous `ReadSchema` optimization, we skipped certain columns of the dataset; now, we skip rows and read only the ones we are interested in (those that satisfy the predicate). We can check if the push-down filter optimization is actually applied by inspecting the query plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PartitionFilters` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file with an additional column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\", round(rand() * 10) mod lit(10) as \"_3\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test: DataFrame = spark.read.parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we want to read data with value `_3` equal to `9.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pushed filter optimization is created, but it would be better if we could just read directly those rows with the exact value for the thrid column. We can achieve that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.mode(\"overwrite\").partitionBy(\"_3\").parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the parquet file is splitted into ten partitions. Now, if we just want to process data with a particular key, Spark will generate an optimun query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testP: DataFrame = spark.read.parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspet the Spark UI to check that we read less data in the last action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
