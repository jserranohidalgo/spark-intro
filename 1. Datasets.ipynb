{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What you already know: HOFs and Scala collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume you already know the basics of Scala and higher-order functions, at least as they are used in the Scala Collections library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// example of List processing with map, filter, etc. \n",
    "\n",
    "List(1,2,3,4)\n",
    "    .map(_ + 1)\n",
    "    .filter(_ % 2 == 0)\n",
    "    .reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: standalone setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a distributed processing framework for transforming big data sets using the computational power of a dedicated cluster. But we can use Spark in an standalone mode (i.e. with no cluster at all), for testing or pedagogical purposes. In that case, we just exploit the cores of your local processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark session\n",
    "\n",
    "The Spark session is the entry point to the Spark interpreter. We need it for running Spark programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/03/04 15:30:58 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/03/04 15:30:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/03/04 15:30:59 INFO SparkContext: Submitted application: a3f47b45-3c83-4d97-b18d-5c96daa8c214\n",
      "20/03/04 15:30:59 INFO SecurityManager: Changing view acls to: jserrano\n",
      "20/03/04 15:30:59 INFO SecurityManager: Changing modify acls to: jserrano\n",
      "20/03/04 15:30:59 INFO SecurityManager: Changing view acls groups to: \n",
      "20/03/04 15:30:59 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/03/04 15:30:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jserrano); groups with view permissions: Set(); users  with modify permissions: Set(jserrano); groups with modify permissions: Set()\n",
      "20/03/04 15:31:01 INFO Utils: Successfully started service 'sparkDriver' on port 51445.\n",
      "20/03/04 15:31:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/03/04 15:31:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/03/04 15:31:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/03/04 15:31:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/03/04 15:31:01 INFO DiskBlockManager: Created local directory at /private/var/folders/0n/mm44trks5_96srf708_htq140000gn/T/blockmgr-56f02d58-b324-4981-8b48-d9b810df31a2\n",
      "20/03/04 15:31:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MB\n",
      "20/03/04 15:31:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/03/04 15:31:02 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "20/03/04 15:31:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.84.117:4043\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0-sources.jar at spark://10.0.84.117:51445/jars/jvm-repr-0.4.0-sources.jar with timestamp 1583332262297\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://10.0.84.117:51445/jars/jvm-repr-0.4.0.jar with timestamp 1583332262300\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1-sources.jar at spark://10.0.84.117:51445/jars/scopt_2.12-3.7.1-sources.jar with timestamp 1583332262301\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar at spark://10.0.84.117:51445/jars/scopt_2.12-3.7.1.jar with timestamp 1583332262312\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.9/1.7.4/ammonite-interp-api_2.12.9-1.7.4-sources.jar at spark://10.0.84.117:51445/jars/ammonite-interp-api_2.12.9-1.7.4-sources.jar with timestamp 1583332262317\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.9/1.7.4/ammonite-interp-api_2.12.9-1.7.4.jar at spark://10.0.84.117:51445/jars/ammonite-interp-api_2.12.9-1.7.4.jar with timestamp 1583332262318\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4-sources.jar at spark://10.0.84.117:51445/jars/ammonite-ops_2.12-1.7.4-sources.jar with timestamp 1583332262319\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4.jar at spark://10.0.84.117:51445/jars/ammonite-ops_2.12-1.7.4.jar with timestamp 1583332262321\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.9/1.7.4/ammonite-repl-api_2.12.9-1.7.4-sources.jar at spark://10.0.84.117:51445/jars/ammonite-repl-api_2.12.9-1.7.4-sources.jar with timestamp 1583332262325\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.9/1.7.4/ammonite-repl-api_2.12.9-1.7.4.jar at spark://10.0.84.117:51445/jars/ammonite-repl-api_2.12.9-1.7.4.jar with timestamp 1583332262332\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4-sources.jar at spark://10.0.84.117:51445/jars/ammonite-util_2.12-1.7.4-sources.jar with timestamp 1583332262339\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4.jar at spark://10.0.84.117:51445/jars/ammonite-util_2.12-1.7.4.jar with timestamp 1583332262343\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7-sources.jar at spark://10.0.84.117:51445/jars/fansi_2.12-0.2.7-sources.jar with timestamp 1583332262349\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7.jar at spark://10.0.84.117:51445/jars/fansi_2.12-0.2.7.jar with timestamp 1583332262350\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8-sources.jar at spark://10.0.84.117:51445/jars/geny_2.12-0.1.8-sources.jar with timestamp 1583332262350\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8.jar at spark://10.0.84.117:51445/jars/geny_2.12-0.1.8.jar with timestamp 1583332262351\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0-sources.jar at spark://10.0.84.117:51445/jars/os-lib_2.12-0.3.0-sources.jar with timestamp 1583332262356\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0.jar at spark://10.0.84.117:51445/jars/os-lib_2.12-0.3.0.jar with timestamp 1583332262357\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5-sources.jar at spark://10.0.84.117:51445/jars/pprint_2.12-0.5.5-sources.jar with timestamp 1583332262359\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5.jar at spark://10.0.84.117:51445/jars/pprint_2.12-0.5.5.jar with timestamp 1583332262373\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7-sources.jar at spark://10.0.84.117:51445/jars/sourcecode_2.12-0.1.7-sources.jar with timestamp 1583332262373\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7.jar at spark://10.0.84.117:51445/jars/sourcecode_2.12-0.1.7.jar with timestamp 1583332262374\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12-sources.jar at spark://10.0.84.117:51445/jars/interface-0.0.12-sources.jar with timestamp 1583332262375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12.jar at spark://10.0.84.117:51445/jars/interface-0.0.12.jar with timestamp 1583332262376\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0-sources.jar at spark://10.0.84.117:51445/jars/scala-collection-compat_2.12-2.0.0-sources.jar with timestamp 1583332262377\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0.jar at spark://10.0.84.117:51445/jars/scala-collection-compat_2.12-2.0.0.jar with timestamp 1583332262378\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0-sources.jar at spark://10.0.84.117:51445/jars/scala-xml_2.12-1.2.0-sources.jar with timestamp 1583332262399\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar at spark://10.0.84.117:51445/jars/scala-xml_2.12-1.2.0.jar with timestamp 1583332262403\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.9/scala-compiler-2.12.9-sources.jar at spark://10.0.84.117:51445/jars/scala-compiler-2.12.9-sources.jar with timestamp 1583332262403\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.9/scala-library-2.12.9-sources.jar at spark://10.0.84.117:51445/jars/scala-library-2.12.9-sources.jar with timestamp 1583332262404\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.9/scala-reflect-2.12.9-sources.jar at spark://10.0.84.117:51445/jars/scala-reflect-2.12.9-sources.jar with timestamp 1583332262405\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.8.2/interpreter-api_2.12-0.8.2-sources.jar at spark://10.0.84.117:51445/jars/interpreter-api_2.12-0.8.2-sources.jar with timestamp 1583332262406\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.8.2/interpreter-api_2.12-0.8.2.jar at spark://10.0.84.117:51445/jars/interpreter-api_2.12-0.8.2.jar with timestamp 1583332262407\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.8.2/jupyter-api_2.12-0.8.2-sources.jar at spark://10.0.84.117:51445/jars/jupyter-api_2.12-0.8.2-sources.jar with timestamp 1583332262407\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.8.2/jupyter-api_2.12-0.8.2.jar at spark://10.0.84.117:51445/jars/jupyter-api_2.12-0.8.2.jar with timestamp 1583332262428\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.9/0.8.2/scala-kernel-api_2.12.9-0.8.2-sources.jar at spark://10.0.84.117:51445/jars/scala-kernel-api_2.12.9-0.8.2-sources.jar with timestamp 1583332262431\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.9/0.8.2/scala-kernel-api_2.12.9-0.8.2.jar at spark://10.0.84.117:51445/jars/scala-kernel-api_2.12.9-0.8.2.jar with timestamp 1583332262435\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Jupyter/kernels/scala212/launcher.jar at spark://10.0.84.117:51445/jars/launcher.jar with timestamp 1583332262439\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.8/scala-reflect-2.12.8.jar at spark://10.0.84.117:51445/jars/scala-reflect-2.12.8.jar with timestamp 1583332262441\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10.jar at spark://10.0.84.117:51445/jars/scala-library-2.12.10.jar with timestamp 1583332262458\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.19.v20190610/jetty-http-9.4.19.v20190610.jar at spark://10.0.84.117:51445/jars/jetty-http-9.4.19.v20190610.jar with timestamp 1583332262465\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.19.v20190610/jetty-server-9.4.19.v20190610.jar at spark://10.0.84.117:51445/jars/jetty-server-9.4.19.v20190610.jar with timestamp 1583332262467\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.19.v20190610/jetty-util-9.4.19.v20190610.jar at spark://10.0.84.117:51445/jars/jetty-util-9.4.19.v20190610.jar with timestamp 1583332262468\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.6.0/almond-spark_2.12-0.6.0.jar at spark://10.0.84.117:51445/jars/almond-spark_2.12-0.6.0.jar with timestamp 1583332262469\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.5.0/ammonite-spark_2.12-0.5.0.jar at spark://10.0.84.117:51445/jars/ammonite-spark_2.12-0.5.0.jar with timestamp 1583332262470\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.19.v20190610/jetty-io-9.4.19.v20190610.jar at spark://10.0.84.117:51445/jars/jetty-io-9.4.19.v20190610.jar with timestamp 1583332262471\n",
      "20/03/04 15:31:02 INFO SparkContext: Added JAR file:/Users/jserrano/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.8/scala-library-2.12.8.jar at spark://10.0.84.117:51445/jars/scala-library-2.12.8.jar with timestamp 1583332262472\n",
      "20/03/04 15:31:02 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/03/04 15:31:02 INFO Executor: Using REPL class URI: http://10.0.84.117:51442\n",
      "20/03/04 15:31:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51447.\n",
      "20/03/04 15:31:02 INFO NettyBlockTransferService: Server created on 10.0.84.117:51447\n",
      "20/03/04 15:31:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/03/04 15:31:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.84.117, 51447, None)\n",
      "20/03/04 15:31:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.84.117:51447 with 912.3 MB RAM, BlockManagerId(driver, 10.0.84.117, 51447, None)\n",
      "20/03/04 15:31:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.84.117, 51447, None)\n",
      "20/03/04 15:31:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.84.117, 51447, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.0.84.117:4043\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1cf025aa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark session in standalone mode\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "\n",
    "val spark: SparkSession = \n",
    "    NotebookSparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging configuration\n",
    "\n",
    "This is convenient to minimize the amount of info displayed in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your first Spark program "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following program is an example of a _Dataset_ program. The Dataset API is one of the languages for distributed data processing that the Spark framework provides. We will omit reference in this notebook to other APIs such as RDDs, DataFrames, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd0.sc:1: value toDS is not a member of List[Int]\n",
      "val res0 = List(1,2,3,4).toDS\n",
      "                         ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "List(1,2,3,4).toDS\n",
    "    .map(_ + 1)\n",
    "    .filter(i => i % 2 == 0)\n",
    "    .reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, this Dataset program does not differ significantly from the Scala collection program. Syntactically, the only difference appears to be the `.toDS` expression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, of course, there are several major differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First difference: performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a heavy computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavyComp(ms: Int = 1000)(x: Int): Int = {\n",
    "  Thread.sleep(ms)\n",
    "  x+1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a way to measure execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run[A](code: => A): A = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(println(\"hola\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Scala Collection program takes some time to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run{\n",
    "    List(1,2,3,4).map(heavyComp(): Int => Int).reduce(_ + _)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the equivalent Dataset program takes half time (or less time depending on the number of cores of your processor)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "    List(1,2,3,4).toDS.map(heavyComp()).reduce(_ + _)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset program run faster because the Spark framework is designed to take advantage of the parallel and distributed architecture of your computing infrastructure. In our case, it simply exploits the number of cores of your processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that using Spark to parallelize your code is overkill. If you are not in a truly distributed setting, you can get along the same benefits more simply using the parallel collections framework of the Scala standard library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(List(1,2,3,4).par.map(heavyComp()).reduce(_ + _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second difference: _laziness_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this Scala collection transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result: List[Int] = List(1,2,3,4).map(_ + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following Dataset one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS.map(_ + 1).filter(_ % 2 == 0).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val program: Dataset[Int] = List(1,2,3,4).toDS.map(_ + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain no transformation at all! Dataset programs are that: _programs_. We won't find any data in an instance of `Dataset`, just a program or _generator_ of a data set. A `Dataset` declares a number of _transformations_ that will be eventually enacted with specific _actions_. For instance, using `collect`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result: Array[Int] = program.collect\n",
    "val result2: Int = program.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or `reduce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result2: Int = program.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Datasets are said to be *lazy*, because we don't inmediately obtain an answer. Rather, we _declaratively_ specify a number of transformations to be applied, and wait until a specific action interprets the transformation program to obtain the desired result. And the same program may be interpreted differently: we may simply want to execute the transformations using `collect`, or may want to perform some calculation using `reduct`. This difference between _transformations_ and _actions_ is reflected very precisely in the [Dataset API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the structure of a Spark program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Dataset` is a program that it's compiled into a lower-level program before it can be actually executed. The compiler of datasets is called _catalyst_. We can inspect the execution plan that is generated for a particular dataset using `explain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS.map(_ + 1).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution plan of a dataset is in turn compiled into an `RDD`, i.e. a lower-level abstraction program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// RDD = Resilient Distributed Dataset\n",
    "val rdd: org.apache.spark.rdd.RDD[Int] = List(1,2,3,4).toDS.map(_ + 1).rdd\n",
    "rdd.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark and functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distinction lies also at the heart of functional programming. On the one hand, there are _programs_ written in a DSL. On the other, there are _interpreters_ that run this program according to different semantics. This is also reflected in the Scala Collections API, particularly, in the notion of _views_. For instance, the following transformation is similarly _lazy_: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// List(1,2,3,4).map(_ + 1).filter(_ % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).view.map(heavyComp())\n",
    "    .filter{ i => \n",
    "        println(\"hola\");\n",
    "        i % 2 == 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation is only executed when we execute the view using, e.g. `toList`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).view.map(heavyComp()).toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SeqView`s are *programs*, much in the same way as `Dataset` objects, whereas `toList` is an *action*, equivalent to the `Dataset` actions `collect` and `reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, Scala _iterators_ are also good examples of *lazyness*. When we create an iterator from a collection, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val it: Iterator[Int] = List(1,2,3,4).iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and specify a number of transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val it2: Iterator[Int] = List(1,2,3,4).iterator\n",
    "    .map{i => println(\"it\"); i + 1}\n",
    "    .filter{ i => println(\"it2\"); i % 2 == 0 }\n",
    "    //.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we don't inmediately obtain those transformations. We are just creating a new iterator that will generate the correspoding data when we ask for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, there is a close relationship between Spark RDDs (the transformation language in which `Dataset`s are actually translated into), and iterators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third difference: the execution framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an action is applied on a `Dataset` program a _job_ is executed by the distributed platform of Spark through a sequence of *stages*; in each stage, the work to be done is performed concurrently by a number of _tasks_. \n",
    "\n",
    "The so-called [Spark UI](http://localhost:4040/) allows us to debug the execution process of all the jobs that are submitted for execution through a given Spark session. For instace, the following action launches a job that can be inspected through the Spark UI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[Int] = List(1,2,3,4).toDS.map(heavyComp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bar in the notebook execution corresponds to one stage of the job exectuion; the X/Y label in each bar indicates the number of tasks already executed (X) and the total number of tasks of that stage (Y). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the work performed by tasks in each partition through `foreachPartition`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.foreachPartition{ it : Iterator[Int] => \n",
    "    println(s\"Task output: \" + it.toList)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this action quite frequently, so let's define an *extension method* for the `Dataset` type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit class DatasetOps[T](ds: Dataset[T]){\n",
    "    def collectPartitions: Unit = \n",
    "        ds.foreachPartition{it : Iterator[T] => println(it.toList)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tasks scheduled for each stage depends on the number of partitions associated to the dataset. When the dataset is first created from a Scala collection, the number of partitions defaults to the number of cores specified when the Spark context was created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4).toDS.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of partitions can be set to a specific value using `repartition`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List(1,2,3,4,5,6,7,8,9,10,11,12).toDS\n",
    "    .repartition(24)\n",
    "    .map(heavyComp(2000))\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling: narrow vs. wide transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a new stage is created when the dataset is repartitioned. More commonly, new stages are created when so-called _wide_ transformations are interpreted. _Narrow_ transformations are those transformations which are not wide: `map`, `filter`, etc. For instance, this program will execute in one stage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following one as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .filter{ t => t._1 == \"a\" }\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this one introduces a new stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Which difference between `filter` and `groupBy` creates such a need for a new stage? And why the next stage generates a dataset with 200 partitions? Let's answer these questions: \n",
    "* First, a new stage is created when data needs to be moved, or *shuffled*, between partitions. \n",
    "* Indeed, this is the case for `groupBy`.\n",
    "* Last, 200 is the default number of partitions created when a shuffled is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value can be customised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contents of the different partitions after each transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), \n",
    "     (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), \n",
    "     (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), \n",
    "     (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do Spark decides where to move the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List((\"a\", 1), (\"e\", 2), (\"f\", 3), (\"d\", 3), (\"z\", 3), (\"k\", 3), (\"i\", 3), (\"o\", 3), (\"l\",2), (\"b\", 2), (\"a\", 3), (\"d\", 3), (\"b\", 4), (\"e\", 4)).toDS\n",
    "    .map{ case (key, value) => (key, value + 1) }\n",
    "    .groupByKey(_._1)\n",
    "    .mapGroups((key, values) => (key, values.toList.map(_._2)))\n",
    "    .explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow or wide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations in the Dataset API can be classified into narrow and wide transformations, systematically. We have already mentioned that `map` and `filter` belong to the former, and `groupByKey` to the latter. What about the following ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `coalesce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = List(1,2,2,3,4,4,5,6,4,7,3,8).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dropDuplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dropDuplicates.collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.flatMap(i => List(i, -i)).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `limit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.limit(6).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mapPartitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mapPartitions{ it: Iterator[Int] => it.map(_ + 1) }.collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `repartition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.repartition(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it with `coalesce`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences can be \"explained\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.repartition(2).explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coalesce(2).explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort(\"value\").collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort(\"value\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about transformations that relate several datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly, information is spread across several datasets, and the Spark Dataset API includes transformations to deal with this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Union`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds1: Dataset[Int] = List(1,2,3,4).toDS.repartition(3)\n",
    "val ds2: Dataset[Int] = List(5,6,7,8).toDS.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.union(ds2).collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No shuffle involved, just a single stage which makes the union of the different partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject\u001b[39m \u001b[36mDS\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mDS._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object DS{\n",
    "    case class Person(name: String, age: Int)\n",
    "    case class Student(name: String, degree: String, year: Int)\n",
    "}\n",
    "\n",
    "import DS._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeople\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPerson\u001b[39m] = [name: string, age: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people: Dataset[Person] = List(\n",
    "    Person(\"Yihui\", 20),\n",
    "    Person(\"Noelia\", 19),\n",
    "    Person(\"Gabriel\", 22),\n",
    "    Person(\"Javier\", 21)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val students: Dataset[Student] = List(\n",
    "    Student(\"Yihui\", \"II\", 2000),\n",
    "    Student(\"Yihui\", \"M\", 2001),\n",
    "    Student(\"Noelia\", \"II\", 2000),\n",
    "    Student(\"Noelia\", \"IS\", 2000),\n",
    "    Student(\"Gabriel\", \"II\", 2004),\n",
    "    Student(\"Javier\", \"II\", 2005),\n",
    "    Student(\"Javier\", \"M\", 2005)).toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.collectPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat unexpectedly, there is no shuffle! This is because Spark performs the join following a \"broadcast\" strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens when one of the datasets is small enough to be copied for each partition. We can force Spark to avoid broadcast as follows (just for testing purposes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.join(students, \"name\").collectPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most distinctive features of Spark is its ability to cache computations of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds5: Dataset[Int] = (0 to 1000).toDS.map(heavyComp(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)\n",
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)\n",
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may instruct the Spark interpreter to not use cached data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds5.unpersist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds5.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `cache` is not a pure transformation but a side-effectful operation. It just instructs the Spark interpreter to cache the dataset as soon as it's executed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds1: Dataset[Int] = List(1,2,3,4).toDS.map(heavyComp())\n",
    "val ds1_cached: Dataset[Int] = ds1.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may expect that the only cached dataset is `ds1_cached`, but that's not true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(ds1.count)\n",
    "run(ds1.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
